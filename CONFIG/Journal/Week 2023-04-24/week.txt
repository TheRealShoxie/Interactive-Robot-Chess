Monday:
    - Started looking into how to get the external camera into the transform tree
        - Got all my cameras into transform tree
        - Placed all the objects into gazebo
        - Had problems with where the depth camera transforms its image to
        - I had to make sure that the link the camera is attached to has x forward as in gazebo coordinates
        - Then had to create a intermidiate frame/link to convert that into a image
        - Based of: https://www.youtube.com/watch?v=A3nw2M47K50&t=1108s
    - Used this webside for possible open_cv image processing algorithms that come with opencv: http://wiki.ros.org/opencv_apps#goodfeature_track
        - Using openCV for edge detection. There is a edgeDetection launch file inside opencv_apps launch files
            - That worked now checking on how to get the cells that I need and check how I can show a image of that so i have a better
            way of debugging it
        - Got typ in meeting to try out opencv_goodfeature_track which tells me points
        - Played around with hough_lines.launch which seems to do well but after using contour moments some cells dont get chosen properly
            - more research or look more into using goodfeature_track as another possible solution

Tuesday:
    - Used goodfeatures again to try to access the points and see if I can write some logic to detect the squares.
      Problem with hough lines into contour moments was that strangely enough some squares didnt get used properly.
      But before I look into goodfeatures I will try to figure out to work with hugh_lines again.
        - Changed to a unordered set for selecting the cells: https://www.geeksforgeeks.org/how-to-create-an-unordered_set-of-user-defined-class-or-struct-in-c/
        - Decided to work with set sorted works the best to get rid of cells.
        - Was able to extract the cells perfectly
        - All based on assumptions that the board is directly underneath the camera, the board doesn't move.
        - To calculate if a cell is occupied I look at all the pixel values for that depth. Then I calc the avarage depth and if the single depth
            is shorter than a required minimum we have a piece there.
        - To calculate what color the piece is I take the center of the x and y values for the depth which should represent the chesspieces center
        - THen I look at the color at that pixel and calculate a singleDigit RGB value and set a cutoff point of white or black

    - On the this webside: https://www.fide.com/FIDE/handbook/Standards_of_Chess_Equipment_and_tournament_venue.pdf
        - On point 2.2 height and weight are defined for standards

    - After spawning in all pawns my current way of using hugh lines doesn't work as the cells arent correctly created anymore due to
        view angles. Will need to look into good features again and see if I cen find a way to deal with

Wednesday:
    - Started looking into good_features and how to use the points to create the squares with an algorithm
    - Starting working on a primitive algorithm to extract the information
    - Extracted information by creating a set of the points which allows me to filter out duplicates and also allows me to order the points
    - Then create rectangles within a certain threshold between points
        - these rectangles represent the chessboard cells and squares
        - Using that I scan the whole cell to get the depth value for the piece if there is open_cv
        - This then gets used to get the color of the chesspieces
        - This now will be published using a rostopic

    - Now need to work on creating a state machine
        - SeverNode and ChessWrapperNode are directly connected rn.
            - This has to be seperated.
            - Insted of ServerNode publishing we make the nodes themselves listen to their respectevile own topic and make the other 
              topics publish to it instead of it publishing to the others
            - Had to rewrite how the communicate now will go back to testing integration manually if it works so far
            - Created a NodeHelper class because both ServerNode and CommandExecuterNode share forwarding of messages.
            - Thus created a helper class which defines forwarding in a generalized term such that it can be used by any node that needs 
              to forward messages and waits for a specific sender if needed.

    - Started working on Creating the Target
        - takes a FEN Move single part and selects the target cells
        - Need to keep in mind that targets should be set before robot arm moves